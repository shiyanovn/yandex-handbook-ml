Классификация $X -> {0, 1, 2, ..., k}$, where k - class number
Регрессия $X -> R$

Линейные функции
$y = w_0 + w_1x_1 + w_2x_2 + ... + w_Dx_D$, where
y - таргет
$x_1, x_2, ..., x_D$ - вектор признаков, фичи
$w_1, w_2, ..., w_D, w_0$ - параметры модели или веса
$w_0$ - сдвиг (bias),
$y = ⟨x, w⟩ + w_0$
Цель найти вектор w
Плюсы:
*  Интерпретируемость ($w_i$ - чем больше, тем больше вклад в результат)
Минусы:
* Если сложная задача, то нужно придумывать дополнительные фичи, которые являются более сложными функциями от исходных (feature engineering)
* Если между признаками есть линейная зависимость, то коэффициенты могут потерять свой физический смысл (решается регуляризацией)
* Веса могут меняться в зависимости от обучающей выборки, хотя, чем больше данных, тем они сходятся к весам наилучшей линейной модели

#### МНК (Метод наименьших квадратов) 
МНК используется для решения задачи регрессии 
Зачастую свободный член $w_0$ опускают, добавляя единицу в вектор x
$y = w_0*1 + w_1x_1 + ... + w_Dx_D =>$
$f_x(x_i) = ⟨w,x_i​⟩$

Функция, которая позволяет узнать насколько часто модель ошибается называется функцией потерь, функционалом качества или лоссом.

$L^p =||y-Xw||_p=(\sum_{i=1}^N(y_i-⟨w,x_i​⟩)^p)^{\frac{1}{p}}$ 

$L^2$ - норма разницы - евклидово расстояние $||y - f_w(x)||_2$ между векторов таргетов и вектором ответом   
$L(f, X, y) =||y - f_w(x)||_2^2$ (squared $L^2$) $=||y-Xw||_2^2=\sum_{i=1}^N(y_i-⟨w,x_i​⟩)^2$          

###### Mean squared error (MSE) или среднеквадратичная ошибка 
MSE$(f,X,y)= \frac{1}{N}||y - Xw||_2^2 = \frac{1}{N}\sum_{i=1}^{N}(y - ⟨w,x_i​⟩)^2$ 

Чтобы найти лучшие веса нужно $||y - Xw||_2^2$ -> min(w)

##### МНК точный аналитический метод
$Xw = w_1x^{(1)} + ... + w_Dx^{(D)}$ 
$L(f, X, y)=(y-Xw)^T(y-Xw)= y^Ty -  y^TXw - X^Tw^Ty+X^Tw^TXw =$ (т.к. $y^TXw$ число => $y^TXw = X^Tw^Ty$ ($(AB)^T = B^TA^T$))$=y^Ty - 2X^Tw^Ty+X^Tw^TXw$  
$∇_w​L(f,X,y) = -2X^Ty + 2X^TXw = 0$
$w = (X^TX)^{-1}X^Ty=(X^TX)^{-1}X^Ty$ 
$O(D^2N+D^3)$ - N длина выборки, D количество признаков у объекта

##### МНК приближенный численный метод
###### Градиентный спуск
$w_j​↦w_j​−α\frac{d}{dw_j​}​L(f_w​,X,y)$ 
$α$ - темп обучения
$L(f_w,X,y) = \frac{1}{N}||Xw-y||^2$ 
$∇_w​L(f,X,y) = ((y -Xw)^T(y-Xw))' = -\frac{2}{N}X^Ty + \frac{2}{N}X^TXw = \frac{2}{N}X^T(Xw-y)$    
* Функция выпуклая => выбор начальной точки влияет на время сходимости, но не настолько сильно, чтобы нельзя было начинать с 0
* Число обусловленности матрицы X существенно влияет на сходимость градиентного спуска, чем более вытянуты эллипсоиды уровня функции потерь, тем хуже
* Темп обучения задается $α$. При неправильном подборе гиперпараметра алгоритм не сходится или вовсе идет в разнос. Другими гипермараметрами являются количество итераций S и/или порог tolerance
Сложность - $O(NDS)$
Сложность по памяти - $O(ND)$

N длина выборки, D количество признаков у объекта
###### Стохаотический градиентный спуск
Так как для градиентного спуска требуется выполнить дорогую операцию вычисления градиента по всей выборке $O(ND)$, то возникает идея градиент его оценкой на подвыборке (batch или mini-batch)
